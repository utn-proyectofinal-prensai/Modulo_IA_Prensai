#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
API Flask para el m√≥dulo de IA de Prensai
Endpoint principal: /procesar-noticias
"""

from flask import Flask, request, jsonify
from functools import wraps
import pandas as pd
import Z_Utils as Z
import O_Utils_Ollama as Oll
import O_Utils_GPT as Gpt
import time
from datetime import timedelta
import logging
import os

app = Flask(__name__)

# Autenticaci√≥n por token para endpoints de configuraci√≥n
VALID_TOKENS = [
    'prensai-config-2025'  # Token √∫nico para acceso a configuraci√≥n
]

def require_api_key(f):
    """
    Decorator que verifica si el request tiene un token v√°lido
    """
    @wraps(f)
    def decorated_function(*args, **kwargs):
        # Obtener el token del header X-API-Key
        token = request.headers.get('X-API-Key')
        
        # Si no hay token, devolver error 401 (No autorizado)
        if not token:
            return jsonify({
                'error': 'Token requerido',
                'message': 'Debes incluir el header X-API-Key para acceder a la configuraci√≥n'
            }), 401
        
        # Si el token no es v√°lido, devolver error 401
        if token not in VALID_TOKENS:
            return jsonify({
                'error': 'Token inv√°lido',
                'message': 'El token proporcionado no es v√°lido'
            }), 401
        
        # Si el token es v√°lido, ejecutar la funci√≥n original
        return f(*args, **kwargs)
    
    return decorated_function

# Los campos ministro_key_words y ministerios_key_words ahora son obligatorios
# Las menciones pueden venir vac√≠as
# Configuraci√≥n configurable en runtime (se puede modificar via endpoints)
RUNTIME_CONFIG = {
    'gpt_active': False,
    'limite_texto': 14900
}

# Campos fijos del DataFrame
CAMPOS_FIJOS = [
    'TITULO', 'TIPO PUBLICACION', 'FECHA', 'SOPORTE', 'MEDIO','SECCION',
    'AUTOR', 'ENTREVISTADO', 'TEMA', 'LINK', 'HTML_OBJ', 'ALCANCE', 'COTIZACION', 'VALORACION',
    'FACTOR POLITICO','TEXTO_PLANO','MENCIONES'
]

def procesar_noticias_con_ia(
    urls: list,
    temas: list,
    menciones: list = None,
    ministro_key_words: list = None,
    ministerios_key_words: list = None
) -> dict:
    """
    Funci√≥n principal que procesa las noticias usando IA
    """
    try:
        # Usar configuraci√≥n de runtime
        gpt_active = RUNTIME_CONFIG['gpt_active']
        limite_texto = RUNTIME_CONFIG['limite_texto']
        
        # Menciones pueden venir vac√≠as (opcional)
        lista_menciones = menciones if menciones else []
        
        # Medici√≥n tiempo de ejecuci√≥n
        t0 = time.time()
        
        # Configurar logger
        Z.setup_logger('Procesamiento_Noticias_API.log')
        
        logging.info(f"Procesando {len(urls)} noticias con API")
        
        # Informar qu√© modelo de IA se usar√°
        modelo_ia = "GPT-4" if gpt_active else "Ollama (llama3.1:8b)"
        print(f"ü§ñ Modelo de IA configurado: {modelo_ia}")
        logging.info(f"Modelo de IA configurado: {modelo_ia}")
        print(f"üìä Configuraci√≥n: GPT_ACTIVE={gpt_active}, LIMITE_TEXTO={limite_texto}")
        logging.info(f"Configuraci√≥n: GPT_ACTIVE={gpt_active}, LIMITE_TEXTO={limite_texto}")
        
        # 1. VALIDAR URLs antes de procesar
        validacion_urls = Z.validar_urls_ejes(urls)
        urls_validas = validacion_urls['validas']
        urls_no_validas = validacion_urls['no_validas']
        
        if not urls_validas:
            logging.warning("No hay URLs v√°lidas para procesar")
            # Convertir motivos de rechazo al formato de errores
            errores = []
            for url in urls_no_validas:
                motivo = validacion_urls['motivos'].get(url, "Error desconocido")
                errores.append({
                    "url": url,
                    "motivo": motivo
                })
            
            # Retornar con c√≥digo de error 422 (datos v√°lidos pero no procesables)
            return {
                "recibidas": len(urls),
                "procesadas": 0,
                "data": [],
                "errores": errores,
                "tiempo_procesamiento": "0:00:00"
            }, 422
        
        # 2. Configurar DataFrame solo con URLs v√°lidas
        df = pd.DataFrame(columns=CAMPOS_FIJOS)
        df['LINK'] = urls_validas
        
        # 3. Extraer texto plano para cada link v√°lido (con reintentos)
        logging.info(f"üîÑ Iniciando extracci√≥n de texto plano para {len(urls_validas)} URLs v√°lidas")
        df['TEXTO_PLANO'] = df['LINK'].apply(lambda x: Z.procesar_link_robusto(x, 'texto', 3))
        
        # 4. Procesar HTML y rellenar campos (con reintentos)
        logging.info(f"üîÑ Iniciando extracci√≥n de HTML para {len(urls_validas)} URLs v√°lidas")
        df['HTML_OBJ'] = df['LINK'].apply(lambda x: Z.procesar_link_robusto(x, 'html', 3))
        
        # 5. VERIFICAR QU√â URLs FALLARON EN LA EXTRACCI√ìN
        logging.info("üîç Verificando URLs que fallaron en la extracci√≥n...")
        urls_extraccion_fallida = []
        for idx, row in df.iterrows():
            if row['TEXTO_PLANO'] is None or row['HTML_OBJ'] is None:
                motivo = "No se pudo extraer contenido (servidor no disponible o contenido vac√≠o)"
                urls_extraccion_fallida.append({
                    'url': row['LINK'],
                    'motivo': motivo
                })
                logging.warning(f"‚ö†Ô∏è URL fall√≥ en extracci√≥n: {row['LINK']} - {motivo}")
        
        # 6. FILTRAR SOLO URLs EXITOSAS para continuar procesamiento
        df_exitosas = df[df['TEXTO_PLANO'].notna() & df['HTML_OBJ'].notna()].copy()
        logging.info(f"‚úÖ URLs exitosas en extracci√≥n: {len(df_exitosas)} de {len(df)}")
        
        if len(df_exitosas) == 0:
            logging.error("‚ùå No se pudo extraer contenido de ninguna URL v√°lida")
            # Convertir motivos de rechazo al formato de errores
            errores = []
            for url in urls_no_validas:
                motivo = validacion_urls['motivos'].get(url, "Error desconocido")
                errores.append({
                    "url": url,
                    "motivo": motivo
                })
            # Agregar errores de extracci√≥n fallida
            errores.extend(urls_extraccion_fallida)
            
            return {
                "recibidas": len(urls),
                "procesadas": 0,
                "data": [],
                "errores": errores,
                "tiempo_procesamiento": "0:00:00"
            }, 500
        
        # 7. Procesar solo URLs exitosas
        logging.info(f"üîÑ Procesando {len(df_exitosas)} URLs exitosas...")
        df_exitosas['TITULO'] = df_exitosas['HTML_OBJ'].apply(Z.get_titulo_from_html_obj)
        df_exitosas['FECHA'] = df_exitosas['HTML_OBJ'].apply(Z.get_fecha_from_html_obj)
        df_exitosas['MEDIO'] = df_exitosas['HTML_OBJ'].apply(Z.get_medio_from_html_obj).apply(Z.normalizar_medio)
        df_exitosas['SOPORTE'] = df_exitosas['HTML_OBJ'].apply(Z.get_soporte_from_html_obj)
        df_exitosas['SECCION'] = df_exitosas['HTML_OBJ'].apply(Z.get_seccion_from_html_obj)
        df_exitosas['COTIZACION'] = df_exitosas['HTML_OBJ'].apply(Z.get_cotizacion_from_html_obj)
        df_exitosas['ALCANCE'] = df_exitosas['HTML_OBJ'].apply(Z.get_alcance_from_html_obj)
        df_exitosas['AUTOR'] = df_exitosas['HTML_OBJ'].apply(Z.get_autor_from_html_obj)
        
        # 8. VERIFICAR CONTENIDO V√ÅLIDO POST-PROCESAMIENTO HTML
        logging.info("üîç Verificando contenido v√°lido post-procesamiento HTML...")
        urls_contenido_invalido = []
        
        for row in df_exitosas.iterrows():
            fecha = row[1]['FECHA']
            cotizacion = row[1]['COTIZACION']
            
            # Verificar si AMBAS son null (contenido inv√°lido)
            if fecha is None and cotizacion is None:  # AMBAS son null
                motivo = "Contenido extra√≠do no es una noticia v√°lida (fecha y cotizaci√≥n son null)"
                urls_contenido_invalido.append({
                    'url': row[1]['LINK'],
                    'motivo': motivo
                })
                logging.warning(f"‚ö†Ô∏è Contenido inv√°lido: {row[1]['LINK']} - FECHA: {fecha}, COTIZACION: {cotizacion}")
        
        # 9. FILTRAR SOLO URLs CON CONTENIDO V√ÅLIDO para IA
        df_contenido_valido = df_exitosas[df_exitosas['LINK'].isin([url for url in df_exitosas['LINK'] if url not in [e['url'] for e in urls_contenido_invalido]])].copy()
        
        logging.info(f"‚úÖ URLs con contenido v√°lido: {len(df_contenido_valido)} de {len(df_exitosas)}")
        
        if len(df_contenido_valido) == 0:
            logging.error("‚ùå No hay URLs con contenido v√°lido para procesar con IA")
            # Combinar todos los errores
            errores = []
            for url in urls_no_validas:
                motivo = validacion_urls['motivos'].get(url, "Error desconocido")
                errores.append({"url": url, "motivo": motivo})
            errores.extend(urls_extraccion_fallida)
            errores.extend(urls_contenido_invalido)
            
            return {
                "recibidas": len(urls),
                "procesadas": 0,
                "data": [],
                "errores": errores,
                "tiempo_procesamiento": "0:00:00"
            }, 500
        
        # 10. Inferencias con IA (solo URLs con contenido v√°lido)
        logging.info(f"ü§ñ Iniciando procesamiento con IA para {len(df_contenido_valido)} URLs v√°lidas...")
        
        # Clasificaci√≥n de tipo de publicaci√≥n (GPT con fallback a Ollama)
        df_contenido_valido['TIPO PUBLICACION'] = df_contenido_valido['TEXTO_PLANO'].apply(
            lambda x: Z.marcar_o_valorar_con_ia(
                x, 
                lambda t: Gpt.clasificar_tipo_publicacion_con_ia(t, ministro_key_words, ministerios_key_words, gpt_active), 
                limite_texto
            )
        )
        
        # Factor pol√≠tico
        df_contenido_valido['FACTOR POLITICO'] = df_contenido_valido['TEXTO_PLANO'].apply(
            lambda x: Z.marcar_o_valorar_con_ia(
                x, 
                Oll.detectar_factor_politico_con_ollama, 
                limite_texto
            )
        )
        
        # Valoraci√≥n
        df_contenido_valido['VALORACION'] = df_contenido_valido['TEXTO_PLANO'].apply(
            lambda x: Z.marcar_o_valorar_con_ia(
                x, 
                lambda t: Gpt.valorar_con_ia(t, ministro_key_words=ministro_key_words, ministerios_key_words=ministerios_key_words, gpt_active=gpt_active), 
                limite_texto
            )
        )
        
        # Clasificaci√≥n de temas
        df_contenido_valido['TEMA'] = df_contenido_valido.apply(
            lambda row: Z.marcar_o_valorar_con_ia(
                row['TEXTO_PLANO'], 
                lambda t: Gpt.clasificar_tema_con_ia(
                    texto=t,
                    lista_temas=temas,
                    tipo_publicacion=row['TIPO PUBLICACION'],
                    gpt_active=gpt_active
                ), 
                limite_texto
            ), 
            axis=1
        )
        
        # 11. Extraer entrevistado
        df_contenido_valido['ENTREVISTADO'] = df_contenido_valido.apply(
            lambda row: Oll.extraer_entrevistado_con_ollama(row['TEXTO_PLANO']) 
            if row['TIPO PUBLICACION'] == 'Entrevista' else None, 
            axis=1
        )
    
        # 12. Detectar menciones solo si se especificaron (solo URLs con contenido v√°lido)
        if menciones:
            df_contenido_valido = Z.buscar_menciones(df_contenido_valido, lista_menciones)
        else:
            # Si no hay menciones, asignar lista vac√≠a
            df_contenido_valido['MENCIONES'] = [[] for _ in range(len(df_contenido_valido))]
        
                
        # 13. Limpiar DataFrame para respuesta
        df_final = df_contenido_valido.drop(columns=['HTML_OBJ', 'TEXTO_PLANO'])
        
        # Medici√≥n tiempo final
        t1 = time.time()
        tiempo_total = str(timedelta(seconds=int(t1 - t0)))
        
        logging.info(f"Procesamiento completado en {tiempo_total}")
        
        # Convertir a JSON
        resultado_json = df_final.to_dict('records')
        
        # 14. Combinar errores de validaci√≥n + extracci√≥n + contenido inv√°lido
        logging.info("üîç Preparando respuesta final con errores combinados...")
        errores = []
        
        # Errores de validaci√≥n previa
        for url in urls_no_validas:
            motivo = validacion_urls['motivos'].get(url, "Error desconocido")
            errores.append({
                "url": url,
                "motivo": motivo
            })
            logging.info(f"‚ùå Error de validaci√≥n: {url} - {motivo}")
        
        # Errores de extracci√≥n fallida
        for error in urls_extraccion_fallida:
            errores.append(error)
            logging.info(f"‚ùå Error de extracci√≥n: {error['url']} - {error['motivo']}")
        
        # Errores de contenido inv√°lido
        for error in urls_contenido_invalido:
            errores.append(error)
            logging.info(f"‚ùå Error de contenido: {error['url']} - {error['motivo']}")
        
        logging.info(f"üìä Resumen final: {len(urls)} recibidas, {len(df_final)} procesadas, {len(errores)} errores")
        
        return {
            "recibidas": len(urls),
            "procesadas": len(df_final),
            "data": resultado_json,
            "errores": errores,
            "tiempo_procesamiento": tiempo_total
        }, 200
        
    except Exception as e:
        logging.error(f"Error en procesamiento: {str(e)}")
        # Convertir URLs a formato de errores
        errores = []
        for url in urls:
            errores.append({
                "url": url,
                "motivo": f"Error en procesamiento: {str(e)}"
            })
        
        return {
            "recibidas": len(urls),
            "procesadas": 0,
            "data": [],
            "errores": errores,
            "tiempo_procesamiento": "0:00:00"
        }, 500

def validar_parametros_noticias(data):
    """
    Funci√≥n reutilizable para validar par√°metros de entrada en endpoints de noticias
    Retorna: (success, error_response, datos_validados)
    """
    try:
        # Validar que venga JSON
        if not request.is_json:
            return False, {
                "error": "Content-Type debe ser application/json"
            }, None
        
        # Validar campos obligatorios
        if 'urls' not in data:
            return False, {
                "error": "Campo 'urls' es obligatorio"
            }, None
        
        if 'temas' not in data:
            return False, {
                "error": "Campo 'temas' es obligatorio"
            }, None
        
        urls = data.get('urls', [])
        temas = data.get('temas', [])
        menciones = data.get('menciones', [])
        ministro_key_words = data.get('ministro_key_words', [])
        ministerios_key_words = data.get('ministerios_key_words', [])
        
        # Validaciones adicionales
        if not urls:
            return False, {
                "error": "La lista de URLs no puede estar vac√≠a"
            }, None
        
        if not temas:
            return False, {
                "error": "La lista de temas no puede estar vac√≠a"
            }, None
        
        # Validar campos obligatorios
        if not ministro_key_words:
            return False, {
                "error": "Campo 'ministro_key_words' es obligatorio"
            }, None
        
        if not isinstance(ministro_key_words, list):
            return False, {
                "error": "Campo 'ministro_key_words' debe ser una lista"
            }, None
        
        if not ministerios_key_words:
            return False, {
                "error": "Campo 'ministerios_key_words' es obligatorio"
            }, None
        
        if not isinstance(ministerios_key_words, list):
            return False, {
                "error": "Campo 'ministerios_key_words' debe ser una lista"
            }, None
        
        # Si todo est√° bien, retornar datos validados
        datos_validados = {
            'urls': urls,
            'temas': temas,
            'menciones': menciones if menciones else None,
            'ministro_key_words': ministro_key_words if ministro_key_words else None,
            'ministerios_key_words': ministerios_key_words if ministerios_key_words else None
        }
        
        return True, None, datos_validados
        
    except Exception as e:
        return False, {
            "error": f"Error en validaci√≥n: {str(e)}"
        }, None

@app.route('/procesar-noticias', methods=['POST'])
def procesar_noticias():
    """
    Endpoint principal para procesar noticias
    """
    try:
        data = request.get_json()
        
        # Usar funci√≥n de validaci√≥n reutilizable
        validacion_ok, error_response, datos_validados = validar_parametros_noticias(data)
        
        if not validacion_ok:
            return jsonify(error_response), 400
        
        # Procesar noticias
        resultado, status_code = procesar_noticias_con_ia(**datos_validados)
        
        # Retornar el resultado con el c√≥digo de estado correspondiente
        return jsonify(resultado), status_code
            
    except Exception as e:
        return jsonify({
            "error": f"Error interno del servidor: {str(e)}"
        }), 500

@app.route('/health', methods=['GET'])
def health_check():
    """
    Endpoint de health check
    """
    return jsonify({
        "status": "OK",
        "service": "Prensai IA API",
        "version": "1.0.0"
    }), 200

@app.route('/config/limite-texto', methods=['POST'])
@require_api_key
def configurar_limite_texto():
    """
    Endpoint para configurar el l√≠mite de texto
    """
    try:
        if not request.is_json:
            return jsonify({
                "error": "Content-Type debe ser application/json"
            }), 400
        
        data = request.get_json()
        nuevo_limite = data.get('limite_texto')
        
        if nuevo_limite is None:
            return jsonify({
                "error": "Campo 'limite_texto' es obligatorio"
            }), 400
        
        if not isinstance(nuevo_limite, int) or nuevo_limite <= 0:
            return jsonify({
                "error": "limite_texto debe ser un n√∫mero entero positivo"
            }), 400
        
        # Actualizar configuraci√≥n
        RUNTIME_CONFIG['limite_texto'] = nuevo_limite
        
        return jsonify({
            "message": f"L√≠mite de texto actualizado a {nuevo_limite}",
            "nuevo_limite": nuevo_limite
        }), 200
        
    except Exception as e:
        return jsonify({
            "error": f"Error interno del servidor: {str(e)}"
        }), 500

@app.route('/config/gpt-active', methods=['POST'])
@require_api_key
def configurar_gpt_active():
    """
    Endpoint para configurar si GPT est√° activo
    """
    try:
        if not request.is_json:
            return jsonify({
                "error": "Content-Type debe ser application/json"
            }), 400
        
        data = request.get_json()
        nuevo_valor = data.get('gpt_active')
        
        if nuevo_valor is None:
            return jsonify({
                "error": "Campo 'gpt_active' es obligatorio"
            }), 400
        
        if not isinstance(nuevo_valor, bool):
            return jsonify({
                "error": "gpt_active debe ser un valor booleano (true/false)"
            }), 400
        
        # Actualizar configuraci√≥n
        RUNTIME_CONFIG['gpt_active'] = nuevo_valor
        
        return jsonify({
            "message": f"GPT Active actualizado a {nuevo_valor}",
            "nuevo_valor": nuevo_valor
        }), 200
        
    except Exception as e:
        return jsonify({
            "error": f"Error interno del servidor: {str(e)}"
        }), 500

@app.route('/config/estado', methods=['GET'])
@require_api_key
def obtener_estado_config():
    """
    Endpoint para obtener el estado actual de la configuraci√≥n
    """
    return jsonify({
        "configuracion": RUNTIME_CONFIG
    }), 200

@app.route('/logs', methods=['GET'])
def obtener_logs():
    """
    Obtiene todo el contenido del archivo de logs de la API
    """
    try:
        log_path = 'Logs/Procesamiento_Noticias_API.log'
        
        if not os.path.exists(log_path):
            return jsonify({
                'error': 'Archivo de logs no encontrado'
            }), 404
            
        with open(log_path, 'r', encoding='utf-8') as f:
            contenido = f.read()
            
        return jsonify({
            'archivo': 'Procesamiento_Noticias_API.log',
            'contenido': contenido,
            'lineas': len(contenido.split('\n'))
        }), 200
        
    except Exception as e:
        return jsonify({
            'error': f'Error al leer logs: {str(e)}'
        }), 500

@app.route('/procesar-noticias-export-excel', methods=['POST'])
def procesar_noticias_export_excel():
    """
    Endpoint para procesar noticias y exportar directamente a Excel
    """
    try:
        data = request.get_json()
        
        # Usar funci√≥n de validaci√≥n reutilizable
        validacion_ok, error_response, datos_validados = validar_parametros_noticias(data)
        
        if not validacion_ok:
            return jsonify(error_response), 400
        
        # Procesar noticias usando la funci√≥n existente
        resultado, status_code = procesar_noticias_con_ia(**datos_validados)
        
        # Si hay error en el procesamiento, retornar el error
        if status_code != 200:
            return jsonify(resultado), status_code
        
        # Si no hay noticias procesadas, no exportar
        if resultado['procesadas'] == 0:
            return jsonify({
                "message": "No hay noticias para exportar",
                "recibidas": resultado['recibidas'],
                "procesadas": 0,
                "data": resultado['data'],
                "errores": resultado['errores'],
                "tiempo_procesamiento": resultado['tiempo_procesamiento'],
                "archivo_excel": None
            }), 422  # 422 = datos v√°lidos pero no procesables
        
        # Crear DataFrame para exportar
        df_export = pd.DataFrame(resultado['data'])
        
        # Agregar campos adicionales para Excel
        df_export['USR_CREADOR'] = 'SOL'
        df_export['USR_REVISOR'] = 'LUNA'
        df_export['CRISIS'] = 'NO'
        
        # Generar nombre de archivo con modelo de IA (se sobrescribe cada vez)
        modelo_ia = "GPT" if RUNTIME_CONFIG['gpt_active'] else "Ollama"
        nombre_archivo = f"Noticias_Procesadas_{modelo_ia}.xlsx"
        ruta_archivo = f"Data_Results/{nombre_archivo}"
        
        # Crear directorio si no existe
        import os
        os.makedirs("Data_Results", exist_ok=True)
        
        # Exportar a Excel
        df_export.to_excel(ruta_archivo, index=False, engine='openpyxl')
        
        logging.info(f"Archivo Excel exportado: {ruta_archivo}")
        
        return jsonify({
            "message": f"Noticias procesadas y exportadas exitosamente a Excel",
            "recibidas": resultado['recibidas'],
            "procesadas": resultado['procesadas'],
            "data": resultado['data'],
            "errores": resultado['errores'],
            "tiempo_procesamiento": resultado['tiempo_procesamiento'],
            "archivo_excel": nombre_archivo,
            "ruta_completa": ruta_archivo,
            "registros_exportados": len(df_export)
        }), 200
        
    except Exception as e:
        logging.error(f"Error en exportaci√≥n a Excel: {str(e)}")
        # Convertir URLs a formato de errores
        urls_error = datos_validados['urls'] if 'datos_validados' in locals() else []
        errores = []
        for url in urls_error:
            errores.append({
                "url": url,
                "motivo": f"Error en exportaci√≥n: {str(e)}"
            })
        
        return jsonify({
            "recibidas": len(urls_error),
            "procesadas": 0,
            "data": [],
            "errores": errores,
            "tiempo_procesamiento": "0:00:00",
            "archivo_excel": None
        }), 500  # 500 = error interno del servidor

if __name__ == '__main__':
    print("üöÄ Iniciando API de Prensai IA...")
    print("üì° Endpoint principal: POST /procesar-noticias")
    print("üìä Exportar a Excel: POST /procesar-noticias-export-excel")
    print("üè• Health check: GET /health")
    print("‚öôÔ∏è  Configuraci√≥n: POST /config/limite-texto, POST /config/gpt-active")
    print("üìã Consultar logs: GET /logs")
    print("üìä Estado config: GET /config/estado")
    print("üîß Puerto: 5000")
    
    app.run(debug=True, host='0.0.0.0', port=5000)
